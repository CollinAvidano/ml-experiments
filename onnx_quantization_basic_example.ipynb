{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ],
      "source": [
        "%pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDuerpduD2GQ",
        "outputId": "fd08c5f0-b831-475b-bafb-cfe00ea30a9d"
      },
      "outputs": [],
      "source": [
        "!sudo apt install valgrind\n",
        "\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install detectors\n",
        "!pip install timm\n",
        "\n",
        "!pip install mlflow\n",
        "\n",
        "!pip install onnx\n",
        "!pip install onnxscript\n",
        "!pip install onnxruntime\n",
        "!pip install netron\n",
        "\n",
        "!pip install torch_tb_profiler\n",
        "!pip install pytorch-lightning\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqgWzTxY6Spd",
        "outputId": "2df6200c-fffc-4ac1-a49a-db4aea68386b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter MLflow uri: http://mlflow.cavidano.com\n",
            "Enter your MLflow username: collin\n",
            "Enter your MLflow password: ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/09/20 23:49:01 INFO mlflow.tracking.fluent: Experiment with name 'onnx_quantization_test5' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mlflow-artifacts:/6/d4f50522faa44b47a9e727b35e39ae2e/artifacts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/09/20 23:49:04 INFO mlflow.tracking._tracking_service.client: 🏃 View run languid-gull-863 at: http://mlflow.cavidano.com/#/experiments/6/runs/d4f50522faa44b47a9e727b35e39ae2e.\n",
            "2024/09/20 23:49:04 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow.cavidano.com/#/experiments/6.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mlflow-artifacts:/6/138583dac56e4334879a806fd0ad3e46/artifacts\n",
            "mlflow-artifacts:/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/09/20 23:49:07 INFO mlflow.tracking._tracking_service.client: 🏃 View run sedate-bass-999 at: http://mlflow.cavidano.com/#/experiments/6/runs/138583dac56e4334879a806fd0ad3e46.\n",
            "2024/09/20 23:49:07 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow.cavidano.com/#/experiments/6.\n"
          ]
        }
      ],
      "source": [
        "#@title MLFlow Tracking Auth\n",
        "import mlflow\n",
        "import os\n",
        "from getpass import getpass\n",
        "import configparser\n",
        "from google.colab import userdata\n",
        "\n",
        "def load_env_from_file(path:str):\n",
        "  # Create a ConfigParser object\n",
        "  config = configparser.ConfigParser()\n",
        "\n",
        "  # Read the INI file\n",
        "  config.read(path)\n",
        "\n",
        "  # Iterate over sections and keys to set environment variables\n",
        "  for section in config.sections():\n",
        "    for key, value in config.items(section):\n",
        "      # Set each key-value pair as an environment variable\n",
        "      os.environ[key.upper()] = value\n",
        "\n",
        "# MLFlow auth\n",
        "mlflow_auth_path = 'mlflow_auth.ini'\n",
        "if os.path.exists(mlflow_auth_path):\n",
        "  load_env_from_file(mlflow_auth_path)\n",
        "else:\n",
        "  try:\n",
        "    os.environ['MLFLOW_TRACKING_URI'] = userdata.get('MLFLOW_TRACKING_URI')\n",
        "    os.environ['MLFLOW_TRACKING_USERNAME'] = userdata.get('MLFLOW_TRACKING_USERNAME')\n",
        "    os.environ['MLFLOW_TRACKING_PASSWORD'] = userdata.get('MLFLOW_TRACKING_PASSWORD')\n",
        "  except userdata.SecretNotFoundError:\n",
        "    os.environ['MLFLOW_TRACKING_URI'] = input('Enter MLflow uri: ')\n",
        "    os.environ['MLFLOW_TRACKING_USERNAME'] = input('Enter your MLflow username: ')\n",
        "    os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your MLflow password: ')\n",
        "    # now proxied\n",
        "    # os.environ['AWS_ACCESS_KEY_ID'] = input('Enter your s3 compatible Identity: ')\n",
        "    # os.environ['AWS_SECRET_ACCESS_KEY'] = getpass('Enter your s3 compatible Key: ')\n",
        "\n",
        "mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
        "\n",
        "experiment = mlflow.set_experiment(\"onnx_quantization_test5\")\n",
        "\n",
        "print(mlflow.get_artifact_uri())\n",
        "\n",
        "mlflow.end_run()\n",
        "with mlflow.start_run():\n",
        "  print(mlflow.get_artifact_uri())\n",
        "  print(experiment.artifact_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uNTd8gR1EkNz"
      },
      "outputs": [],
      "source": [
        "#@title Wrapper for calibration data loader so onnx can use it\n",
        "\n",
        "#add work dir to args later rely on user to handle cleanup\n",
        "import torch\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from onnxruntime import quantization\n",
        "import numpy as np\n",
        "\n",
        "# Wrapper for calibration data loader so onnx can use it\n",
        "class QuantizationDataReader(quantization.CalibrationDataReader):\n",
        "  def __init__(self, torch_dl, input_name):\n",
        "    self.torch_dl = torch_dl\n",
        "    self.input_name = input_name\n",
        "    self.datasize = len(self.torch_dl)\n",
        "    self.enum_data = iter(self.torch_dl)\n",
        "\n",
        "  def to_numpy(self, pt_tensor):\n",
        "    return pt_tensor.detach().cpu().numpy() if pt_tensor.requires_grad else pt_tensor.cpu().numpy()\n",
        "\n",
        "  def get_next(self):\n",
        "    batch = next(self.enum_data, None)\n",
        "    if batch is not None:\n",
        "      return {self.input_name: self.to_numpy(batch[0])}\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def rewind(self):\n",
        "    self.enum_data = iter(self.torch_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def onnx_convert(model_pt, output_path, torch_input:torch.tensor, training=False)-> None:\n",
        "  # if it contains batch norms double check they arent causing accuracy issues at eval time.\n",
        "  #If so run this in training mode\n",
        "\n",
        "  torch.onnx.export(model_pt,\n",
        "                    torch_input,\n",
        "                    output_path,\n",
        "                    export_params=True,\n",
        "                    opset_version=14,\n",
        "                    do_constant_folding=(not training),\n",
        "                    training=(torch.onnx.TrainingMode.TRAINING if training else torch.onnx.TrainingMode.Eval),\n",
        "                    input_names = ['input'],\n",
        "                    output_names = ['output'],\n",
        "                    dynamic_axes={'input' : {0 : 'batch_size'},\n",
        "                                  'output' : {0 : 'batch_size'}})\n",
        "\n",
        "def onnx_quantize(init_model_path:str, model_name:str, calibration_data_loader:torch.utils.data.DataLoader)->str:\n",
        "  ort_provider = ['CUDAExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n",
        "  ort_sess = ort.InferenceSession(init_model_path, providers=ort_provider)\n",
        "\n",
        "  # load and preprocess\n",
        "  model_onnx = onnx.load(init_model_path)\n",
        "  onnx.checker.check_model(model_onnx)\n",
        "  model_prep_path = f'{model_name}_prep.onnx'\n",
        "  quantization.shape_inference.quant_pre_process(init_model_path, model_prep_path, skip_symbolic_shape=False)\n",
        "\n",
        "  qdr = QuantizationDataReader(calibration_data_loader, input_name=ort_sess.get_inputs()[0].name)\n",
        "\n",
        "  # actual quantization\n",
        "  model_int8_path = f'{model_name}_int8.onnx'\n",
        "  q_static_opts = {\"ActivationSymmetric\": torch.cuda.is_available(), \"WeightSymmetric\":True}\n",
        "  quantization.quantize_static(model_input=model_prep_path,\n",
        "                                                model_output=model_int8_path,\n",
        "                                                calibration_data_reader=qdr,\n",
        "                                                extra_options=q_static_opts)\n",
        "  return model_int8_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64gyn_SyE6J4",
        "outputId": "36a5f364-a405-46b4-d43b-a89f26b3d312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 16278068.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "#@title Torch setup\n",
        "import torch\n",
        "from torch import optim, nn,  utils, Tensor\n",
        "from itertools import repeat\n",
        "from torchsummary import summary\n",
        "from torchvision import models, transforms\n",
        "import torchvision\n",
        "\n",
        "batch_size = 128\n",
        "dataset = torchvision.datasets.CIFAR10(root=\"./data\", download=True, transform=transforms.ToTensor())\n",
        "calib_ds, test_ds = torch.utils.data.random_split(dataset, [0.5, 0.5])\n",
        "\n",
        "# shortening test_ds again\n",
        "test_ds = torch.utils.data.Subset(test_ds, range(0, 1000))\n",
        "\n",
        "calibration_data_loader = utils.data.DataLoader(calib_ds, batch_size=batch_size, shuffle=False)\n",
        "test_data_loader = utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ducBYzTDDyWQ",
        "outputId": "ff39a38e-c6bf-4107-9dbc-24a3a9adafe1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://huggingface.co/edadaltocg/resnet18_cifar10/resolve/main/pytorch_model.bin\" to /root/.cache/torch/hub/checkpoints/resnet18_cifar10.pth\n",
            "100%|██████████| 42.7M/42.7M [00:03<00:00, 13.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'bn1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer1.0.bn1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer1.0.bn2.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer1.1.bn1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer1.1.bn2.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer2.0.bn1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer2.0.bn2.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer2.0.downsample.1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer2.1.bn1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer2.1.bn2.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer3.0.bn1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer3.0.bn2.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer3.0.downsample.1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer3.1.bn1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer3.1.bn2.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer4.0.bn1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer4.0.bn2.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer4.0.downsample.1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer4.1.bn1.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:668: UserWarning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: 'layer4.1.bn2.num_batches_tracked'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:350.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n"
          ]
        }
      ],
      "source": [
        "#@title Model download and quantization\n",
        "# https://huggingface.co/edadaltocg/resnet18_cifar10\n",
        "import detectors\n",
        "import timm\n",
        "model = timm.create_model(\"resnet18_cifar10\", pretrained=True)\n",
        "\n",
        "#I NEED TO FIX THIS IT SHOULD BE ABLE TO WORK WITHOUT BEING IN TRAIN MODE\n",
        "# dont remember how fixed this conversion?.... but batchnorm used to be causing an issue...\n",
        "# OH THATS RIGHT HAD TO PUT IT IN TRAIN MODE WHICH IS NOT RIGHTHA yeahhhh\n",
        "\n",
        "# nuke batchnorm from orbit\n",
        "# hoping if onnx has no value for these it will fallback to per batch norms just like pytorch itself will\n",
        "# def stop(m):\n",
        "#   if isinstance(m, nn.BatchNorm2d):\n",
        "#     m.track_running_stats = False\n",
        "#     m.running_mean = None\n",
        "#     m.running_var = None\n",
        "# model.eval()\n",
        "# model.apply(lambda m: stop(m))\n",
        "\n",
        "\n",
        "# Thats Fun...\n",
        "# UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'batch_norm' is set to train=True. Exporting with train=True.\n",
        "\n",
        "# pretrained via model method\n",
        "# model = torch.hub.load(\"pytorch/vision\", \"resnet18\", weights=\"IMAGENET1K_V1\")\n",
        "# model.eval()\n",
        "\n",
        "converted_model_path = \"onnx_res18.onnx\"\n",
        "torch_input = torch.randn(batch_size, 3, 32, 32) # has to match dataset and of course model\n",
        "onnx_convert(model, converted_model_path, torch_input, training=True)\n",
        "\n",
        "onnx_q_model_path = onnx_quantize(converted_model_path, \"resnet18\", calibration_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWiuGITDFuym",
        "outputId": "39078fc6-1061-4315-e61f-7c3eb13fbddd"
      },
      "outputs": [],
      "source": [
        "# conversion and quantization always gives paths so have to reload...\n",
        "onnx_model = onnx.load(converted_model_path)\n",
        "onnx_q_model = onnx.load(onnx_q_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title reload and log onnx models BROKEN\n",
        "import logging\n",
        "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
        "np_in = to_numpy(torch_input)\n",
        "\n",
        "mlflow.onnx.log_model(onnx_model, \"mlflow-artifacts:/onnx_res18.onnx\")\n",
        "mlflow.onnx.log_model(onnx_q_model, \"mlflow-artifacts:/onnx_res18_int8.onnx\")\n",
        "\n",
        "# mlflow.onnx.save_model(onnx_model, \"onnx_res18.onnx\")\n",
        "# mlflow.onnx.save_model(onnx_q_model, \"onnx_res18_int8.onnx\")\n",
        "\n",
        "# mlflow.onnx.log_model(onnx_model, \"onnx_res18.onnx\")\n",
        "\n",
        "# mlflow.onnx.log_model(onnx_model, \"onnx_res18.onnx\", input_example = np_in)\n",
        "# mlflow.onnx.log_model(onnx_q_model, \"onnx_res18_int8.onnx\", input_example = np_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LdBhBYWYTuE"
      },
      "outputs": [],
      "source": [
        "#@title load onnx models from registry BROKEN\n",
        "# should save to same file names\n",
        "converted_model_path = \"onnx_res18.onnx\"\n",
        "onnx_q_model_path = \"onnx_res18_int8.onnx\"\n",
        "\n",
        "# pretty sure I need run ids for this to work\n",
        "mlflow.onnx.load_model(\"mlflow-artifacts:/onnx_res18.onnx\", dst_path=\"./\")\n",
        "mlflow.onnx.load_model(\"mlflow-artifacts:/onnx_res18_int8.onnx\", dst_path=\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnWob8eAbJ5R"
      },
      "outputs": [],
      "source": [
        "def to_numpy(tensor):\n",
        "  return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZVGSw0VDwFS"
      },
      "outputs": [],
      "source": [
        "#@title Eval code onnx model inits and helpers\n",
        "\n",
        "def run_pytorch(model, data):\n",
        "  return model(data)\n",
        "\n",
        "def run_onnx(session, data):\n",
        "  ort_inputs = {session.get_inputs()[0].name: to_numpy(data)}\n",
        "  return session.run(None, ort_inputs)[0]\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.to('cuda')\n",
        "model.eval()\n",
        "\n",
        "ort_provider = ['CUDAExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n",
        "ort_converted_sess = ort.InferenceSession(converted_model_path, providers=ort_provider)\n",
        "ort_quantized_sess = ort.InferenceSession(onnx_q_model_path, providers=ort_provider)\n",
        "\n",
        "num_threads = torch.get_num_threads()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e7fVs7wl7V1Z"
      },
      "outputs": [],
      "source": [
        "#@title COMPARE ALL FORMS ACCURACY\n",
        "from tqdm import tqdm\n",
        "\n",
        "correct_pt = 0\n",
        "correct_converted_onnx = 0\n",
        "correct_quantized_onnx = 0\n",
        "pt_to_conv_tot_abs_error = 0\n",
        "conv_to_quant_tot_abs_error = 0\n",
        "\n",
        "for img_batch, label_batch in tqdm(test_data_loader, ascii=True, unit=\"batches\"):\n",
        "\n",
        "  # pytorch\n",
        "  if torch.cuda.is_available():\n",
        "    img_batch = img_batch.to('cuda')\n",
        "    label_batch = label_batch.to('cuda')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    pt_outs = model(img_batch)\n",
        "\n",
        "  pt_preds = torch.argmax(pt_outs, dim=1)\n",
        "  correct_pt += torch.sum(pt_preds == label_batch)\n",
        "\n",
        "  # converted\n",
        "  ort_converted_outs = run_onnx(ort_converted_sess, img_batch)\n",
        "  ort_preds = np.argmax(ort_converted_outs, axis=1)\n",
        "  correct_converted_onnx += np.sum(np.equal(ort_preds, to_numpy(label_batch)))\n",
        "\n",
        "  # quantized\n",
        "  ort_quantized_outs = run_onnx(ort_quantized_sess, img_batch)\n",
        "  ort_preds = np.argmax(ort_quantized_outs, axis=1)\n",
        "  correct_quantized_onnx += np.sum(np.equal(ort_preds, to_numpy(label_batch)))\n",
        "\n",
        "  # abs errors\n",
        "  pt_to_conv_tot_abs_error += np.sum(np.abs(to_numpy(pt_outs - ort_converted_outs)))\n",
        "  conv_to_quant_tot_abs_error += np.sum(np.abs(ort_converted_outs - ort_quantized_outs))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"pytorch   top-1 acc = {100.0 * correct_pt/len(test_ds)} with {correct_pt} correct samples\")\n",
        "print(f\"converted top-1 acc = {100.0 * correct_converted_onnx/len(test_ds)} with {correct_converted_onnx} correct samples\")\n",
        "print(f\"quantized top-1 acc = {100.0 * correct_quantized_onnx/len(test_ds)} with {correct_quantized_onnx} correct samples\")\n",
        "\n",
        "mae = pt_to_conv_tot_abs_error/(len(test_ds))\n",
        "print(f\"pt_to_conv: mean abs error = {mae} with total abs error {pt_to_conv_tot_abs_error}\")\n",
        "\n",
        "mae = conv_to_quant_tot_abs_error/(len(test_ds))\n",
        "print(f\"conv_to_quant: mean abs error = {mae} with total abs error {conv_to_quant_tot_abs_error}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEt9Bh4O_aNt"
      },
      "outputs": [],
      "source": [
        "#@title timing with torch.benchmark\n",
        "import torch.utils.benchmark as benchmark\n",
        "\n",
        "results = []\n",
        "label = \"Average Inference Times\"\n",
        "\n",
        "num_runs = 100\n",
        "batch_sizes = [1, 64, 128, 512]\n",
        "batch_sizes = [32]\n",
        "\n",
        "def timing(t):\n",
        "  # return t.timeit(num_runs)\n",
        "  return t.blocked_autorange(min_run_time=4)\n",
        "\n",
        "with mlflow.start_run():\n",
        "  for batch_size in batch_sizes:\n",
        "    mlflow.log_param(\"batch_size\", batch_size)\n",
        "    data = torch.randn((batch_size, 3, 32, 32))\n",
        "    np_data = to_numpy(data)\n",
        "\n",
        "    sub_label = f\"Batch Size: {batch_size}\"\n",
        "\n",
        "    # pytorch\n",
        "    t = benchmark.Timer(\n",
        "      stmt = 'run_pytorch(model, data)',\n",
        "      setup = 'from __main__ import run_pytorch',\n",
        "      globals={'model': model, 'data': data},\n",
        "      num_threads=num_threads,\n",
        "      label=label,\n",
        "      sub_label=sub_label,\n",
        "      description=\"Pytorch\",\n",
        "    )\n",
        "    results.append(timing(t))\n",
        "    mlflow.log_metric(\"time\", str(results[-1]))\n",
        "\n",
        "    # converted\n",
        "    t = benchmark.Timer(\n",
        "      stmt = 'run_onnx(session, data)',\n",
        "      setup = 'from __main__ import run_onnx',\n",
        "      globals={'session': ort_converted_sess, 'data': data},\n",
        "      num_threads=num_threads,\n",
        "      label=label,\n",
        "      sub_label=sub_label,\n",
        "      description=\"Onnx Converted\",\n",
        "    )\n",
        "    results.append(timing(t))\n",
        "    mlflow.log_metric(\"time\", str(results[-1]))\n",
        "\n",
        "    # quantized\n",
        "    t = benchmark.Timer(\n",
        "      stmt = 'run_onnx(session, data)',\n",
        "      setup = 'from __main__ import run_onnx',\n",
        "      globals={'session': ort_quantized_sess, 'data': data},\n",
        "      num_threads=num_threads,\n",
        "      label=label,\n",
        "      sub_label=sub_label,\n",
        "      description=\"Onnx Quantized\"\n",
        "    )\n",
        "    results.append(timing(t))\n",
        "    mlflow.log_metric(\"time\", str(results[-1]))\n",
        "\n",
        "  compare = benchmark.Compare(results)\n",
        "  compare.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOJlrzVv5UuI"
      },
      "outputs": [],
      "source": [
        "#@title timing with perf_counter\n",
        "import time\n",
        "\n",
        "results = []\n",
        "num_runs = 100\n",
        "batch_size = 32\n",
        "data = torch.zeros((batch_size, 3, 32, 32))\n",
        "\n",
        "def perf_benchmark(model, input_data, eval_func, num_runs=num_runs):\n",
        "    times = []\n",
        "    for _ in range(num_runs):\n",
        "        start = time.perf_counter()\n",
        "        eval_func(model, input_data)\n",
        "        end = time.perf_counter()\n",
        "        times.append(end - start)\n",
        "    return np.mean(times), np.std(times)\n",
        "\n",
        "print(\"perf_counter\")\n",
        "print(\"pytorch\")\n",
        "mean, std = perf_benchmark(model, data, run_pytorch)\n",
        "print('mean: {}ms, std: {}ms'.format(mean * 1000, std * 1000))\n",
        "\n",
        "print(\"converted\")\n",
        "mean, std = perf_benchmark(ort_converted_sess, data, run_onnx)\n",
        "print('mean: {}ms, std: {}ms'.format(mean * 1000, std * 1000))\n",
        "\n",
        "print(\"quantized\")\n",
        "mean, std = perf_benchmark(ort_quantized_sess, data, run_onnx)\n",
        "print('mean: {}ms, std: {}ms'.format(mean * 1000, std * 1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APP3EegoCgwZ"
      },
      "outputs": [],
      "source": [
        "#@title callgrind collection with torch.benchmark\n",
        "import os\n",
        "\n",
        "results = []\n",
        "label = \"Average Inference Times\"\n",
        "\n",
        "num_runs = 100\n",
        "batch_sizes = [1, 64, 128, 512]\n",
        "# batch_sizes = [1]\n",
        "\n",
        "def timing(t):\n",
        "  return t.collect_callgrind()\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "  data = torch.randn((batch_size, 3, 32, 32))\n",
        "  np_data = to_numpy(data)\n",
        "  sub_label = f\"Batch Size: {batch_size}\"\n",
        "\n",
        "  # pytorch\n",
        "  t = benchmark.Timer(\n",
        "    stmt = 'run_pytorch(model, data)',\n",
        "    setup = 'from __main__ import run_pytorch',\n",
        "    globals={'model': model, 'data': data},\n",
        "    num_threads=num_threads,\n",
        "    label=label,\n",
        "    sub_label=sub_label,\n",
        "    description=\"Pytorch\",\n",
        "  )\n",
        "  results.append(timing(t))\n",
        "\n",
        "  # converted\n",
        "  t = benchmark.Timer(\n",
        "    stmt = 'run_onnx(session, data)',\n",
        "    setup = 'from __main__ import run_onnx',\n",
        "    globals={'session': ort_converted_sess, 'data': np_data},\n",
        "    num_threads=num_threads,\n",
        "    label=label,\n",
        "    sub_label=sub_label,\n",
        "    description=\"Onnx Converted\",\n",
        "  )\n",
        "  results.append(timing(t))\n",
        "\n",
        "  # quantized\n",
        "  t = benchmark.Timer(\n",
        "    stmt = 'run_onnx(session, data)',\n",
        "    setup = 'from __main__ import run_onnx',\n",
        "    globals={'session': ort_quantized_sess, 'data': np_data},\n",
        "    num_threads=num_threads,\n",
        "    label=label,\n",
        "    sub_label=sub_label,\n",
        "    description=\"Onnx Quantized\"\n",
        "  )\n",
        "  results.append(timing(t))\n",
        "\n",
        "for result in results:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFC_B6XixyYn"
      },
      "outputs": [],
      "source": [
        "#@title torch profiler\n",
        "import torch.profiler\n",
        "\n",
        "batch_size = 32\n",
        "data = torch.zeros((batch_size, 3, 32, 32))\n",
        "\n",
        "with torch.profiler.profile(\n",
        "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
        "        record_shapes=True,\n",
        "        profile_memory=True,\n",
        "        with_stack=True\n",
        ") as prof:\n",
        "  for i in range(5):\n",
        "    run_pytorch(model, data)\n",
        "    prof.step()\n",
        "\n",
        "  print(prof.key_averages().table(row_limit=-1))\n",
        "  # print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC7O0BWW06l6"
      },
      "outputs": [],
      "source": [
        "#@title onnx profiler\n",
        "import onnxruntime as rt\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "options = rt.SessionOptions()\n",
        "options.enable_profiling = True\n",
        "\n",
        "ort_provider = ['CUDAExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n",
        "\n",
        "batch_size = 32\n",
        "data = torch.zeros((batch_size, 3, 32, 32))\n",
        "\n",
        "ort_converted_sess = ort.InferenceSession(onnx_q_model_path, sess_options=options, providers=ort_provider)\n",
        "run_onnx(ort_converted_sess, data)\n",
        "profile_file = ort_converted_sess.end_profiling()\n",
        "print(\"converted\")\n",
        "print(profile_file)\n",
        "# with open(profile_file, 'r') as f:\n",
        "#   pprint.pp(json.load(f))\n",
        "\n",
        "\n",
        "ort_quantized_sess = ort.InferenceSession(onnx_q_model_path, sess_options=options, providers=ort_provider)\n",
        "run_onnx(ort_quantized_sess, data)\n",
        "profile_file = ort_quantized_sess.end_profiling()\n",
        "print(\"quantized\")\n",
        "print(profile_file)\n",
        "# with open(profile_file, 'r') as f:\n",
        "#   pprint.pp(json.load(f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHlc5Y76Ed5A"
      },
      "outputs": [],
      "source": [
        "# netron viewer\n",
        "import os\n",
        "import torch\n",
        "import netron\n",
        "import portpicker\n",
        "from google.colab import output\n",
        "\n",
        "# model should come from another block\n",
        "# output_path = \"/content/output.pth\"\n",
        "# torch.save(model.state_dict(), output_path)\n",
        "output_path = converted_model_path\n",
        "port = portpicker.pick_unused_port()\n",
        "\n",
        "# Read the model file and start the netron browser.\n",
        "with output.temporary():\n",
        "  netron.start(output_path, port, browse=True)\n",
        "\n",
        "output.serve_kernel_port_as_iframe(port, height='800')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
