{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install speedtest-cli\n",
        "!speedtest-cli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8aFj0Euv8PO",
        "outputId": "6c8f61a8-4d42-4c19-8fb8-36360728e115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  speedtest-cli\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 24.1 kB of archives.\n",
            "After this operation, 106 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 speedtest-cli all 2.1.3-2 [24.1 kB]\n",
            "Fetched 24.1 kB in 0s (217 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package speedtest-cli.\n",
            "(Reading database ... 123620 files and directories currently installed.)\n",
            "Preparing to unpack .../speedtest-cli_2.1.3-2_all.deb ...\n",
            "Unpacking speedtest-cli (2.1.3-2) ...\n",
            "Setting up speedtest-cli (2.1.3-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Retrieving speedtest.net configuration...\n",
            "Testing from Google Cloud (34.48.203.211)...\n",
            "Retrieving speedtest.net server list...\n",
            "Selecting best server based on ping...\n",
            "Hosted by All Points Broadband (Ashburn, VA) [42.39 km]: 6.849 ms\n",
            "Testing download speed................................................................................\n",
            "Download: 1188.00 Mbit/s\n",
            "Testing upload speed......................................................................................................\n",
            "Upload: 741.20 Mbit/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MbogNVniGYf",
        "outputId": "0d0bece1-c6b4-46f7-c6e8-5195c09d8dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n",
            "Collecting torch_tb_profiler\n",
            "  Downloading torch_tb_profiler-0.4.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torch_tb_profiler) (2.0.3)\n",
            "Requirement already satisfied: tensorboard!=2.1.0,>=1.15 in /usr/local/lib/python3.10/dist-packages (from torch_tb_profiler) (2.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->torch_tb_profiler) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->torch_tb_profiler) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->torch_tb_profiler) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->torch_tb_profiler) (1.25.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.2.2)\n",
            "Installing collected packages: torch_tb_profiler\n",
            "Successfully installed torch_tb_profiler-0.4.3\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.2.5-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.11.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->pytorch-lightning) (12.5.40)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.2 pytorch-lightning-2.2.5 torchmetrics-1.4.0.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install optuna\n",
        "\n",
        "!pip install torch_tb_profiler\n",
        "!pip install pytorch-lightning\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D75KWHhwKoJS"
      },
      "outputs": [],
      "source": [
        "# GRAPHING AND FUN STUFF\n",
        "from torch.profiler import *\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# nice data table exploration in collab https://colab.research.google.com/notebooks/data_table.ipynb#scrollTo=jcQEX_3vHOUz\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv5RgFJ3a_H8",
        "outputId": "3bcc0aa5-76e3-44a6-ecaa-877ba9ab79bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /content/image-net/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:01<00:00, 89436344.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/image-net/cifar-100-python.tar.gz to /content/image-net\n"
          ]
        }
      ],
      "source": [
        "# DATASET LOAD\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "from torch import utils\n",
        "import os\n",
        "\n",
        "trans = transforms.Compose([\n",
        "  transforms.Resize((128,128)),\n",
        "  transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# TODO should do if gpu check\n",
        "kwargs = { \"pin_memory\": True, \"num_workers\": os.cpu_count() }\n",
        "dataset_train = CIFAR100(root=\"/content/image-net\", download=True, transform=trans)\n",
        "dataset_test = CIFAR100(root=\"/content/image-net\", train=False, transform=trans)\n",
        "train_loader = utils.data.DataLoader(dataset_train, **kwargs)\n",
        "test_loader = utils.data.DataLoader(dataset_test, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAlsVbueSA4q"
      },
      "outputs": [],
      "source": [
        "# COMMON\n",
        "import torch\n",
        "from torch import optim, nn,  utils, Tensor\n",
        "from itertools import repeat\n",
        "\n",
        "\n",
        "#helper literally just to ensure all biases are false as in the pytorch implmentation\n",
        "def conv2d_helper(in_channels, out_channels, kernel_size, stride, padding):\n",
        "  return nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=True)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, dropout_percent=0.5, special_i_skip=False, half_res=False):\n",
        "    super().__init__()\n",
        "    self.block = nn.Sequential(\n",
        "      conv2d_helper(in_channels, out_channels, kernel_size, stride=(2 if half_res else 1), padding=1),\n",
        "      nn.BatchNorm2d(out_channels), # from other example\n",
        "      nn.ReLU(),\n",
        "      conv2d_helper(out_channels, out_channels, kernel_size, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(out_channels), # from other example\n",
        "      # nn.Dropout(p=dropout_percent) # from other example\n",
        "    )\n",
        "    self.skip_connection = conv2d_helper(in_channels, out_channels, kernel_size=1, stride=2, padding=0) if half_res else nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return nn.functional.relu(self.block(x) + self.skip_connection(x))\n",
        "\n",
        "def create_meta_block(in_channels, out_channels, repeats, half_res=True):\n",
        "  # every conv is a 3x3 kernel\n",
        "  # error to call with less than 1\n",
        "  return nn.Sequential(\n",
        "    ResBlock(in_channels, out_channels, kernel_size=3, half_res=half_res),\n",
        "    *repeat(ResBlock(out_channels, out_channels, kernel_size=3), repeats-1)\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF1oVqEMRAAq"
      },
      "outputs": [],
      "source": [
        "# PYTORCH MODULE\n",
        "class NativeResNet34(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    init_conv = nn.Sequential(\n",
        "      conv2d_helper(in_channels=3, out_channels=64, kernel_size=7,\n",
        "                stride=2, padding=3),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    )\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      init_conv,\n",
        "      create_meta_block(64, 64, 3, False),\n",
        "      create_meta_block(64, 128, 4),\n",
        "      create_meta_block(128, 256, 6),\n",
        "      create_meta_block(256, 512, 3)\n",
        "    )\n",
        "    self.lin = nn.Linear(in_features=512, out_features=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    map = self.model(x)\n",
        "    pooled_map = nn.functional.avg_pool2d(map, kernel_size=map.shape[-2:], stride=1)\n",
        "    pooled_map = torch.flatten(pooled_map, start_dim=1)\n",
        "    scores = self.lin(pooled_map)\n",
        "    return scores\n",
        "\n",
        "\n",
        "\n",
        "class NativeResNet18(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    init_conv = nn.Sequential(\n",
        "      conv2d_helper(in_channels=3, out_channels=64, kernel_size=7,\n",
        "                stride=2, padding=3),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    )\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      init_conv,\n",
        "      create_meta_block(64, 64, 2, False),\n",
        "      create_meta_block(64, 128, 2),\n",
        "      create_meta_block(128, 256, 2),\n",
        "      create_meta_block(256, 512, 2)\n",
        "    )\n",
        "    self.lin = nn.Linear(in_features=512, out_features=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    map = self.model(x)\n",
        "    pooled_map = nn.functional.avg_pool2d(map, kernel_size=map.shape[-2:], stride=1)\n",
        "    pooled_map = torch.flatten(pooled_map, start_dim=1)\n",
        "    scores = self.lin(pooled_map)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp68EJ3JK9AH"
      },
      "outputs": [],
      "source": [
        "### LIGHTNING WRAPPER\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class LightningResNet(pl.LightningModule):\n",
        "  def __init__(self, backbone, learning_rate=0.1, batch_size=1024):\n",
        "    super().__init__()\n",
        "    # saves all args as lightning hyperparams\n",
        "    # this saves them to the checkpoint and other logging mechanisms AND makes them accessible as self.ARG\n",
        "    self.save_hyperparameters()\n",
        "    self.backbone = backbone\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.backbone(x)\n",
        "\n",
        "  # MUST RETURN THE LOSS\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    scores = self.forward(x)\n",
        "    loss = self.loss(scores, y)\n",
        "\n",
        "    # Logging to TensorBoard (if installed) by default\n",
        "    self.log('train_loss', loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    scores = self.forward(x)\n",
        "    loss = self.loss(scores, y)\n",
        "\n",
        "    # calculate acc\n",
        "    labels_hat = torch.argmax(scores, dim=1)\n",
        "    val_acc = torch.sum(y == labels_hat).item() / (float(len(y)))\n",
        "\n",
        "    # log the outputs!\n",
        "    self.log_dict({'val_loss': loss, 'val_acc': val_acc})\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    scores = self.forward(x)\n",
        "    loss = self.loss(scores, y)\n",
        "\n",
        "    # calculate acc\n",
        "    labels_hat = torch.argmax(scores, dim=1)\n",
        "    test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
        "\n",
        "    # log the outputs!\n",
        "    self.log_dict({'test_loss': loss, 'test_acc': test_acc})\n",
        "\n",
        "\n",
        "  # MUST RETURN THE OPTIMIZER\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "    scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
        "    return optimizer\n",
        "    # can return a Dictionary, with an \"optimizer\" key, and (optionally) a \"lr_scheduler\" key whose value is a single LR scheduler or lr_scheduler_config.\n",
        "\n",
        "  def train_dataloader(self): #\n",
        "    return utils.data.DataLoader(dataset_train, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  def val_dataloader(self): #\n",
        "    return utils.data.DataLoader(dataset_train, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  def test_dataloader(self): #\n",
        "    return utils.data.DataLoader(dataset_test, batch_size=self.hparams.batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkk2zgSMHb_Y",
        "outputId": "077f7729-39a1-49e9-f547-4cd17d466fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/parsing.py:199: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n"
          ]
        }
      ],
      "source": [
        "model = LightningResNet(learning_rate=1e-1, backbone=NativeResNet18(num_classes=1000))\n",
        "\n",
        "# from torchsummary import summary\n",
        "# summary(model.cuda(), (3, 128, 128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPHjMgrxk7wo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "cb8b4d98-6b7b-45be-c468-44c26ead27bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3ca824a3f350>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# TODO APPEND DATE TO CHECKPOINT DIR or just start actually using mlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdrive_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         )\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "### TRAIN DEFAULT\n",
        "from pytorch_lightning.loggers import tensorboard\n",
        "\n",
        "gdrive_dir = \"/content/gdrive/\"\n",
        "checkpoint_dir = f\"{gdrive_dir}/MyDrive/checkpoints\"\n",
        "\n",
        "# TODO APPEND DATE TO CHECKPOINT DIR or just start actually using mlflow\n",
        "from google.colab import drive\n",
        "drive.mount(gdrive_dir)\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {checkpoint_dir}\n",
        "\n",
        "# may be default_root_dir\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_dir, save_top_k=3, monitor=\"val_loss\")\n",
        "# note as I learned if the logger has a default dir it will prefer that OVER DEFAULT ROOT DIR but manually setting dirpath fixes that\n",
        "# https://github.com/Lightning-AI/pytorch-lightning/blob/90d04b5b86f37994cdceccc6de32f0e93b1cc7f0/src/lightning/pytorch/callbacks/model_checkpoint.py#L623\n",
        "trainer = Trainer(callbacks=[checkpoint_callback], log_every_n_steps=10, default_root_dir=checkpoint_dir)\n",
        "\n",
        "resume_checkpoint = \"/content/gdrive/MyDrive/checkpoints/epoch=15-step=784.ckpt\"\n",
        "# # automatically restores model, epoch, step, LR schedulers, etc...\n",
        "trainer.fit(model, ckpt_path=resume_checkpoint)\n",
        "\n",
        "# trainer.fit(model)\n",
        "\n",
        "# loaders as part of module should seperate to datamodule at some point\n",
        "\n",
        "checkpoint_callback.best_model_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.tuner import Tuner\n",
        "# Auto-scale batch size with binary search power actually its much faster to find ok result\n",
        "# tuner = Tuner(trainer)\n",
        "# tuner.scale_batch_size(model, mode=\"power\")\n",
        "\n",
        "# Fit as normal with new batch size\n",
        "# trainer.fit(model)\n",
        "# trainer.fit(model, train_dataloaders=train_loader)"
      ],
      "metadata": {
        "id": "xeS9hGHSrrkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR0M99zooG4j"
      },
      "outputs": [],
      "source": [
        "# native train loop\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = NativeResNet18()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "# Here, we use enumerate(training_loader) instead of\n",
        "# iter(training_loader) so that we can track the batch\n",
        "# index and do some intra-epoch reporting\n",
        "for i, data in tqdm(enumerate(train_loader)):\n",
        "    # Every data instance is an input + label pair\n",
        "    inputs, labels = data\n",
        "\n",
        "    # Zero your gradients for every batch!\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Make predictions for this batch\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Compute the loss and its gradients\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    # Adjust learning weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Gather data and report\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "        last_loss = running_loss / 1000 # loss per batch\n",
        "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "        tb_x = epoch_index * len(train_loader) + i + 1\n",
        "        tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "        running_loss = 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBS09bHWepN2"
      },
      "outputs": [],
      "source": [
        "### OPTUNA\n",
        "import optuna\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # Suggest a learning rate\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "\n",
        "    # Set model's learning rate at creation\n",
        "    model = LightningResNet(learning_rate = lr, batch_size = 2048)\n",
        "\n",
        "    # Assuming you have a DataLoader instance named `train_dataloader`\n",
        "    trainer = Trainer(max_epochs=1, limit_train_batches=1000, limit_val_batches=None, limit_test_batches=None)\n",
        "    trainer.fit(model, train_loader)\n",
        "\n",
        "    global results\n",
        "    results = trainer.test(model, test_loader)\n",
        "    # Return the validation loss or any other metric you want to optimize\n",
        "    return results[\"test_loss\"]\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Print the best trial\n",
        "print(study.best_trial.params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLH4XGAx11Om"
      },
      "outputs": [],
      "source": [
        "print(len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMjBVXPH0UIv"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}