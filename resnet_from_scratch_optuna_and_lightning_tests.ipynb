{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MbogNVniGYf",
        "outputId": "0d0bece1-c6b4-46f7-c6e8-5195c09d8dc7"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install optuna\n",
        "\n",
        "!pip install torch_tb_profiler\n",
        "!pip install pytorch-lightning\n",
        "# fix for collab env\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D75KWHhwKoJS"
      },
      "outputs": [],
      "source": [
        "# GRAPHING AND FUN STUFF\n",
        "from torch.profiler import *\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# nice data table exploration in collab https://colab.research.google.com/notebooks/data_table.ipynb#scrollTo=jcQEX_3vHOUz\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv5RgFJ3a_H8",
        "outputId": "3bcc0aa5-76e3-44a6-ecaa-877ba9ab79bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /content/image-net/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169001437/169001437 [00:01<00:00, 89436344.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /content/image-net/cifar-100-python.tar.gz to /content/image-net\n"
          ]
        }
      ],
      "source": [
        "# DATASET LOAD\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "from torch import utils\n",
        "import os\n",
        "\n",
        "trans = transforms.Compose([\n",
        "  transforms.Resize((128,128)),\n",
        "  transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# TODO should do if gpu check\n",
        "kwargs = { \"pin_memory\": True, \"num_workers\": os.cpu_count() }\n",
        "dataset_train = CIFAR100(root=\"/content/image-net\", download=True, transform=trans)\n",
        "dataset_test = CIFAR100(root=\"/content/image-net\", train=False, transform=trans)\n",
        "train_loader = utils.data.DataLoader(dataset_train, **kwargs)\n",
        "test_loader = utils.data.DataLoader(dataset_test, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAlsVbueSA4q"
      },
      "outputs": [],
      "source": [
        "# COMMON RESNET UTILITIES\n",
        "import torch\n",
        "from torch import optim, nn,  utils, Tensor\n",
        "from itertools import repeat\n",
        "\n",
        "\n",
        "#helper literally just to ensure all biases are false as in the pytorch implmentation\n",
        "def conv2d_helper(in_channels, out_channels, kernel_size, stride, padding):\n",
        "  return nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=True)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, dropout_percent=0.5, special_i_skip=False, half_res=False):\n",
        "    super().__init__()\n",
        "    self.block = nn.Sequential(\n",
        "      conv2d_helper(in_channels, out_channels, kernel_size, stride=(2 if half_res else 1), padding=1),\n",
        "      nn.BatchNorm2d(out_channels), # from other example\n",
        "      nn.ReLU(),\n",
        "      conv2d_helper(out_channels, out_channels, kernel_size, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(out_channels), # from other example\n",
        "      # nn.Dropout(p=dropout_percent) # from other example\n",
        "    )\n",
        "    self.skip_connection = conv2d_helper(in_channels, out_channels, kernel_size=1, stride=2, padding=0) if half_res else nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return nn.functional.relu(self.block(x) + self.skip_connection(x))\n",
        "\n",
        "def create_meta_block(in_channels, out_channels, repeats, half_res=True):\n",
        "  # every conv is a 3x3 kernel\n",
        "  # error to call with less than 1\n",
        "  return nn.Sequential(\n",
        "    ResBlock(in_channels, out_channels, kernel_size=3, half_res=half_res),\n",
        "    *repeat(ResBlock(out_channels, out_channels, kernel_size=3), repeats-1)\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF1oVqEMRAAq"
      },
      "outputs": [],
      "source": [
        "# RESNET FROM SCRATCH IMPLEMENTATION IN PYTORCH\n",
        "class NativeResNet34(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    init_conv = nn.Sequential(\n",
        "      conv2d_helper(in_channels=3, out_channels=64, kernel_size=7,\n",
        "                stride=2, padding=3),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    )\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      init_conv,\n",
        "      create_meta_block(64, 64, 3, False),\n",
        "      create_meta_block(64, 128, 4),\n",
        "      create_meta_block(128, 256, 6),\n",
        "      create_meta_block(256, 512, 3)\n",
        "    )\n",
        "    self.lin = nn.Linear(in_features=512, out_features=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    map = self.model(x)\n",
        "    pooled_map = nn.functional.avg_pool2d(map, kernel_size=map.shape[-2:], stride=1)\n",
        "    pooled_map = torch.flatten(pooled_map, start_dim=1)\n",
        "    scores = self.lin(pooled_map)\n",
        "    return scores\n",
        "\n",
        "class NativeResNet18(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    init_conv = nn.Sequential(\n",
        "      conv2d_helper(in_channels=3, out_channels=64, kernel_size=7,\n",
        "                stride=2, padding=3),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    )\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      init_conv,\n",
        "      create_meta_block(64, 64, 2, False),\n",
        "      create_meta_block(64, 128, 2),\n",
        "      create_meta_block(128, 256, 2),\n",
        "      create_meta_block(256, 512, 2)\n",
        "    )\n",
        "    self.lin = nn.Linear(in_features=512, out_features=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    map = self.model(x)\n",
        "    pooled_map = nn.functional.avg_pool2d(map, kernel_size=map.shape[-2:], stride=1)\n",
        "    pooled_map = torch.flatten(pooled_map, start_dim=1)\n",
        "    scores = self.lin(pooled_map)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp68EJ3JK9AH"
      },
      "outputs": [],
      "source": [
        "### LIGHTNING WRAPPER\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class LightningResNet(pl.LightningModule):\n",
        "  def __init__(self, backbone, learning_rate=0.1, batch_size=1024):\n",
        "    super().__init__()\n",
        "    # saves all args as lightning hyperparams\n",
        "    # this saves them to the checkpoint and other logging mechanisms AND makes them accessible as self.ARG\n",
        "    self.save_hyperparameters()\n",
        "    self.backbone = backbone\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.backbone(x)\n",
        "\n",
        "  # MUST RETURN THE LOSS\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    scores = self.forward(x)\n",
        "    loss = self.loss(scores, y)\n",
        "\n",
        "    # Logging to TensorBoard (if installed) by default\n",
        "    self.log('train_loss', loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    scores = self.forward(x)\n",
        "    loss = self.loss(scores, y)\n",
        "\n",
        "    # calculate acc\n",
        "    labels_hat = torch.argmax(scores, dim=1)\n",
        "    val_acc = torch.sum(y == labels_hat).item() / (float(len(y)))\n",
        "\n",
        "    # log the outputs!\n",
        "    self.log_dict({'val_loss': loss, 'val_acc': val_acc})\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    scores = self.forward(x)\n",
        "    loss = self.loss(scores, y)\n",
        "\n",
        "    # calculate acc\n",
        "    labels_hat = torch.argmax(scores, dim=1)\n",
        "    test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
        "\n",
        "    # log the outputs!\n",
        "    self.log_dict({'test_loss': loss, 'test_acc': test_acc})\n",
        "\n",
        "\n",
        "  # MUST RETURN THE OPTIMIZER\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "    scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
        "    return optimizer\n",
        "    # can return a Dictionary, with an \"optimizer\" key, and (optionally) a \"lr_scheduler\" key whose value is a single LR scheduler or lr_scheduler_config.\n",
        "\n",
        "  def train_dataloader(self): #\n",
        "    return utils.data.DataLoader(dataset_train, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  def val_dataloader(self): #\n",
        "    return utils.data.DataLoader(dataset_train, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  def test_dataloader(self): #\n",
        "    return utils.data.DataLoader(dataset_test, batch_size=self.hparams.batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkk2zgSMHb_Y",
        "outputId": "077f7729-39a1-49e9-f547-4cd17d466fc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/parsing.py:199: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n"
          ]
        }
      ],
      "source": [
        "model = LightningResNet(learning_rate=1e-1, backbone=NativeResNet18(num_classes=1000))\n",
        "\n",
        "# from torchsummary import summary\n",
        "# summary(model.cuda(), (3, 128, 128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {checkpoint_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "sPHjMgrxk7wo",
        "outputId": "cb8b4d98-6b7b-45be-c468-44c26ead27bc"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3ca824a3f350>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# TODO APPEND DATE TO CHECKPOINT DIR or just start actually using mlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdrive_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         )\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "### TRAIN DEFAULT\n",
        "from pytorch_lightning.loggers import tensorboard\n",
        "\n",
        "# may be default_root_dir\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_dir, save_top_k=3, monitor=\"val_loss\")\n",
        "# note as I learned if the logger has a default dir it will prefer that OVER DEFAULT ROOT DIR but manually setting dirpath fixes that\n",
        "# https://github.com/Lightning-AI/pytorch-lightning/blob/90d04b5b86f37994cdceccc6de32f0e93b1cc7f0/src/lightning/pytorch/callbacks/model_checkpoint.py#L623\n",
        "trainer = Trainer(callbacks=[checkpoint_callback], log_every_n_steps=10)\n",
        "\n",
        "# # automatically restores model, epoch, step, LR schedulers, etc...\n",
        "# trainer.fit(model, ckpt_path=resume_checkpoint)\n",
        "\n",
        "trainer.fit(model)\n",
        "\n",
        "# loaders as part of module should seperate to datamodule at some point\n",
        "checkpoint_callback.best_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeS9hGHSrrkX"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.tuner import Tuner\n",
        "# Auto-scale batch size with binary search power actually its much faster to find ok result\n",
        "# tuner = Tuner(trainer)\n",
        "# tuner.scale_batch_size(model, mode=\"power\")\n",
        "\n",
        "# Fit as normal with new batch size\n",
        "# trainer.fit(model)\n",
        "# trainer.fit(model, train_dataloaders=train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR0M99zooG4j"
      },
      "outputs": [],
      "source": [
        "# native train loop\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = NativeResNet18()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "# Here, we use enumerate(training_loader) instead of\n",
        "# iter(training_loader) so that we can track the batch\n",
        "# index and do some intra-epoch reporting\n",
        "for i, data in tqdm(enumerate(train_loader)):\n",
        "    # Every data instance is an input + label pair\n",
        "    inputs, labels = data\n",
        "\n",
        "    # Zero your gradients for every batch!\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Make predictions for this batch\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Compute the loss and its gradients\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    # Adjust learning weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Gather data and report\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "        last_loss = running_loss / 1000 # loss per batch\n",
        "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "        tb_x = epoch_index * len(train_loader) + i + 1\n",
        "        tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "        running_loss = 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBS09bHWepN2"
      },
      "outputs": [],
      "source": [
        "### OPTUNA\n",
        "import optuna\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # Suggest a learning rate\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "\n",
        "    # Set model's learning rate at creation\n",
        "    model = LightningResNet(learning_rate = lr, batch_size = 2048)\n",
        "\n",
        "    # Assuming you have a DataLoader instance named `train_dataloader`\n",
        "    trainer = Trainer(max_epochs=1, limit_train_batches=1000, limit_val_batches=None, limit_test_batches=None)\n",
        "    trainer.fit(model, train_loader)\n",
        "\n",
        "    results = trainer.test(model, test_loader)\n",
        "    # Return the validation loss or any other metric you want to optimize\n",
        "    return results[\"test_loss\"]\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the best trial\n",
        "print(study.best_trial.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMjBVXPH0UIv"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
